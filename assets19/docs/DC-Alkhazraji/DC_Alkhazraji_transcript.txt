American Sign Language is a primary means of communication for over 500,000 people in the U.S., and many people who are Deaf or Hard of Hearing prefer to receive information in the form of American Sign Language. Unfortunately, few websites display contents in American Sign Language; one challenge is that videos of human American Sign Language signers would be difficult to update and maintain when information on a website must change. The human would need to be re-recorded. We therefore investigate technology to automate the creation of animations of American Sign Language based on easy-to-update script.We used motion-capture data recorded from humans to train machine learning models to predict realistic timing parameters for American Sign Language animation, based on the sentence syntax and other features. I am investigating the following research questions:1. Would adding pauses during American Sign Language animations improve the understandability of the message?  2. How much pause time do we need to add between signs, due to the syntactic phrase structure or sentence boundaries?3. How does the speed of signing vary during sentences, and how can we produce animations with realistic timing?I have built predictive models for modeling where to insert prosodic breaks (pauses), adjusting the pause durations for these pauses, and adjusting differential signing rate for American Sign Language animations.I am using two evaluation approaches: 1. A cross-validation study using human recordings, where my model out-performed a state-of-the-art rule-based model. 2. A user study where American Sign Language native signers provided subjective feedback after viewing animations generated by our models.My ultimate goal is to build software that can generate understandable American Sign Language animations of a virtual human signer automatically from an easy-to-update script.