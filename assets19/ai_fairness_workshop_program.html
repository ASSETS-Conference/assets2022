<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="Content-Type" content="text/html;" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ORKSHOP - AI FAIRNESS WORKSHOP Program | ACM ASSETS 2019 - PITTSBURGH, PA, USA (October 28th to 30th, 2019)</title>
    <link href='https://fonts.googleapis.com/css?family=Raleway:200,400,700' rel='stylesheet' type='text/css'>
    <link href="styles.css" rel="stylesheet" type="text/css" /> 
    <script src="https://code.jquery.com/jquery-2.2.4.js"></script>
    <script>
        localStorage.setItem("current_page", "Workshop");
    </script>
</head>

<body>
    <div id="header"></div>
    
    <div id="content">
        <h1 tabindex="0">
            ACM ASSETS 2019 Workshop on AI Fairness for People with Disabilities
        </h1>

        <p><b>Sunday, October 27, 2019</b></p>
        <p><b>Monongahela Room, Omni William Penn Hotel, Pittsburgh, PA</b></p>

        <p><b>Sponsored by IBM Research and IBM Design</b></p>
        <p>THIS WORKSHOP IS FULL.</p>
        <p>This workshop is not part of the ASSETS conference program. It is fully booked. We cannot accept new registrations, but contact <a href="mailto:aiworkshop-assets19@acm.org">aiworkshop-assets19@acm.org</a> if you would like to be added to the waiting list. Position papers from the workshop will appear in the October 2019 and June 2020 issues of the SIGACCESS Newsletter.</p>        

        <p><b>8:30 a.m.:</b> Registration, Poster Set-Up. Pre-workshop breakfast generously provided by Microsoft.</p>

        <p><b>9:00 a.m.:</b> Welcoming Remarks from Workshop Organizers, and Attendee Introductions</p>

        <p><b>9:30 a.m.:</b> Keynote talk by <a href="https://www.law.georgetown.edu/faculty/alexandra-givens/" target="_blank">Alexandra Givens</a>, Executive Director of the Institute for Technology Law & Policy at Georgetown University 

            <div id="workshop_keynote">

                <img src="images/AlexandraGivens.jpg" style="float:left;width:400px;margin-right:30px;" alt="Photo of Alexandra Givens.">

                <p>
                    Alexandra Reeve Givens is the Executive Director of Georgetown Law’s Institute for Technology Law & Policy. The Institute educates students and produces law and policy research on issues such as inclusive innovation, the use of technology to promote access to justice, and how policymakers should respond to the opportunities and challenges presented by new technologies. It has recently launched a multi-year project on algorithmic fairness and the rights of people with disabilities. Alexandra previously served as the Chief Counsel for Intellectual Property and Antitrust on the Senate Judiciary Committee, where she worked on issues relating to innovation and consumer protection. She began her career as a litigator at Cravath, Swaine & Moore in New York City, after graduating from Yale University and Columbia Law School. In addition to her role at Georgetown, Alexandra serves as Vice Chair of the Christopher and Dana Reeve Foundation, which is dedicated to funding innovative research and improving quality of life for the millions of people living with paralysis. Alexandra was 11 when her father, Christopher Reeve, experienced a life-altering spinal cord injury that caused him to be paralyzed from the neck down. She has served on the Board of Directors since 2006.
                </p>

                <p><b>The Legal Framework: Assessing the Legal Tools to Address Algorithmic Fairness for People with Disabilities</b></p>

                <p>
                    Abstract: Recent years have seen a surge in conversations about fairness, accountability and transparency in the use of algorithmic systems. Mainstream newspapers describe the flaws of hiring tools that perpetuate inequity by selecting candidates who resemble current employees. Researchers have underscored the disparate impact of “risk assessment” scores that assess a criminal defendant’s likelihood of reoffending using proxies for income and race. Increasingly, researchers, advocates, and—to some extent—the broader public are raising alarm about the ways in which algorithmic systems can replicate, mask, codify and scale existing biases. The potential impact of such systems on disabled people has so far received much less attention.
                </p>

                <p>
                    This presentation examines the handful of court cases that have begun to challenge algorithmic systems for discrimination and other harms. It explores the legal theories that will likely underpin future challenges, including by describing how current companies adopting such systems seem to be interpreting the legal risks. It then presents several distinct ways in which these legal theories fall short in upholding the rights and interests of people with disabilities.  Drawing on these vulnerabilities, the presentation calls for a research agenda that, among other things:
                </p>

                <p><ul>
                    <li>more fully documents the ways in which algorithmic systems may exclude or adversely impact people with disabilities; </li>
                    <li>addresses how the diversity of disabilities, and many people’s understandable reluctance to disclose their disabilities, complicate efforts to detect bias and respond to it; </li>
                    <li>recognizes that people’s right to privacy in matters of personal health stands in particular conflict with algorithms that harm people based on inferences about their perceived health or ability; </li>
                    <li>grapples with fundamental questions about whether predictive models can ever adequately consider individuals who are statistical “outliers”, or whose diversity of circumstances defy category-based treatment;</li>
                    <li>interrogates the objectives, outcomes, and inherent trade-offs involved in using algorithmic systems, doing so in a way that centers the interests of the user-subject, not just the entity using the system; and</li>
                    <li>examines the obligations (legal and non-legal) of entities designing and deploying algorithmic systems, the rights of user-subjects of these systems, and how those obligations and rights can be protected and enforced. </li>
                </ul></p>

                <p>
                    The decision of whether and how to design, deploy, and monitor algorithmic systems should involve far more than legal obligations, bringing in questions of ethics, distributive justice, agency and more. Nevertheless, the legal framework provides relevant context for such conversations, and illuminates key areas where further research and analysis is required.
                </p>

            </div>

        </p>

        <p><b>10:15 a.m.:</b> Poster Session + Coffee Break</p>

        <p>List of Posters: (Poster abstracts are available on the <a href="ai_fairness_workshop_abstracts.html#posters">Workshop Abstracts page</a>.)</p>
        <ul>
            <li><b>“Learning to Say No: When FATE is too Late”</b> <i>by Rua M. Williams (University of Florida)</i></li>
            <li><b>“Perspectives of People with Essential Tremors on the Privacy of Adaptive Assistive Technologies”</b> <i>by Foad Hamidi, Kellie Poneres, Aaron Massey, Amy Hurst (University of Maryland Baltimore County and NYU)  </i></li>
            <li><b>“AI-Assisted UI Design for Blind and Low-Vision Creators”</b> <i>by Venkatesh Potluri, Tadashi Grindeland, Jon E. Froehlich and Jennifer Mankoff (University of Washington)</i></li>
            <li><b>“Using Computational Ethnography to Enhance Curation of Real-world Data (RWD) of Individuals Living with Chronic Pain and Invisible Disability”</b> <i>by Rhonda J. Moore, Ross Smith, Qi Liu, Rebecca Racz, and Phaedra Boinodiris (U.S. Food and Drug Administration and University College Dublin)</i></li>
            <li><b>“Stranded at the Edges and Falling through the Cracks?”</b> <i>by Jutta Treviranus (Ontario College of Art and Design University) </i></li>
            <li><b>“Facial Analysis Models Do Not Perform Well on Faces of Individuals with Dementia”</b> <i>by Babak Taati, Azin Asgarian, Shun Zhao, Siavash Rezaei, Ahmed B. Ashraf, M. Erin Browne, Kenneth M. Prkachin, Alex Mihailidis, and Thomas Hadjistavropoulos (University of Toronto)</i></li>
            <li><b>“Artificial Intelligence: The Importance of Labels and Diversity among Deaf and Hard of Hearing People”</b> <i>by Raja Kushalnagar (Gallaudet University)</i></li>
            <li><b>“Fairness in Data Collection to Train Machine Learning Models for Persons with Disabilities in Africa”</b> <i>by Jefferson Sankara (Lori Systems LTD)</i></li>
        </ul>

        <p><b>10:45 a.m.:</b> Short Talks (x5) (Talk abstracts are available on the <a href="ai_fairness_workshop_abstracts.html#shorttalks">Workshop Abstracts page</a>.)</p>
        <ul>
            <li><b>“Artificial Intelligence Fairness in the Context of Accessibility Research on Intelligent Systems for People who are Deaf or Hard of Hearing”</b> <i>by Sushant Kafle, Abraham Glasser, Sedeeq Al-khazraji, Larwan Berke, Matthew Seita, and Matt Huenerfauth (Golisano College of Computing & Information Sciences and Rochester Institute of Technology)</i></li>
            <li><b>“What is the Point of Fairness? Disability, AI and The Complexity of Justice”</b> <i></i>by Cynthia Bennett and Os Keyes (University of Washington) </li>
            <li><b>“Distributive Justice and Disability in Machine Learning”</b> <i>by Alan Lundgard (MIT)</i></li>
            <li><b>“Artificial Intelligence and the Dignity of Risk”</b> <i>by Emily Shea Tanis and Clayton Lewis (Coleman Institute for Cognitive Disabilities)</i></li>
            <li><b>“Fairness of AI for People with Disabilities: Problem Analysis and Interdisciplinary Collaboration”</b> <i>by Jason J.G. White (Educational Testing Service)</i></li>
        </ul>

        <p><b>12:00 p.m.:</b> Catered Lunch (Sponsored by IBM Research)</p>

        <p><b>1:30 p.m.:</b> Short Talks (x4) (Talk abstracts are available on the <a href="ai_fairness_workshop_abstracts.html#shorttalks">Workshop Abstracts page</a>.)</p>
        <ul>
            <li><b>“Fairness Issues in AI Systems that Augment Sensory Abilities”</b> <i>by Leah Findlater, Steven Goodman, Yuhang Zhao, Shiri Azenkot, and Margot Hanley (University of Washington and Cornell Tech)</i></li>
            <li><b>“Toward Fairness in AI for People with Disabilities: A Research Roadmap”</b> <i></i>by Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach, and Meredith Ringel Morris (Microsoft Research and Carnegie Mellon University)</li>
            <li><b>“Designing Accessible, Explainable AI (XAI) Experiences”</b> <i>by Christine T. Wolf and Kathryn E. Ringland (IBM Research Almaden and Northwestern University)</i></li>
            <li><b>“Unintended Machine Learning Biases as Social Barriers for Persons with Disabilities”</b> <i>by Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Yu Zhong, and Stephen Denuyl (Google)</i></li>
        </ul>

        <p><b>2:30 p.m.:</b> Coffee Break</p>

        <p><b>2:45 p.m.:</b> Breakout Sessions (Topics TBD based on Workshop Attendees’ Interests)</p>

        <p><b>3:45 p.m.:</b> Closing Remarks from Workshop Organizers</p>

        <p><b>4:00 p.m.:</b> Workshop End</p>

        <br>

        <p>Note: There will be a related evening event attendees may wish to sign up for. This is not part of the workshop. Information is below:</p>

        <p><b>7:00 p.m.:</b> SIGACCESS Sponsored Event: <a href="https://www.bricolagepgh.org/programs/program-listings/projectamelia/" target="_blank">Project Amelia</a>, an immersive theater experience based around the launch of a groundbreaking new AI product. Registration for this event is now open. Workshop and ASSETS 2019 attendees should have received a registration link by email. If you are missing this email, please contact <a href="mailto:aiworkshop-assets19@acm.org">aiworkshop-assets19@acm.org</a>.</p>
    </div><!-- eof content-->

    <div id="footer"></div>

    <script type="text/javascript">
        $(document).ready(function () {
            $('#header').load('header.html');
            //$('#content').load('soon.html');
            //$('#footer').load('footer.html');
        });
    </script>
</body>

</html>

